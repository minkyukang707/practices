'머신러닝 실무 프로젝트'에서 발췌 (한빛 미디어) 

2.1 머신러닝 알고리즘 선택법
- 분류: 정답이 비연속적인 클래스(카테고리)이며, 정답과 입력 데이터의 조합을 학습하여 새로운 데이터의 클래스를 예측 
- 회귀: 정답이 수치다. 정답과 입력 데이터의 조합을 학습하여 새로운 데이터에서 연속하는 값을 예측
- 군집화: 어떤 기준에 따라 데이터를 그룹으로 묶는다
- 차원축소: 시각화 또는 계산량 절감을 목적으로 고차원 데이터를 저차원 공간에 매핑한다
- 그 외:
- 추천: 사용자가 선호할 만한 품목, 혹은 열람한 것과 비슷한 품목을 제시
- 이상탐지: 수상한 접근 등 평소와 다른 행동을 검출'
- 고빈도 패턴 마이닝: 데이터 안에서 발생 빈도가 높은 패턴을 추출
- 강화 학습: 바둑이나 장기처럼 정답이 불분명한 환경에서 앞으로 취할 행동을 선택하는 방법 학습

+흐름도(본교제확인)

2.2 분류
분류(classification) 비연속적인 값을 예측한다. 
클래스 수 2개: 이진 분류 / 3개 이상: 다중 분류 

분류 종류:
퍼셉트론
로지스틱 회귀
서포트 벡터 머신(SVM)
신경망
k-최근접이웃(k-NN)
결정트리
랜덤 포레스트
경사 부스팅 결정트리(GBDT)

퍼셉트론,로지스틱 회귀, SVM, 신경망은 두 클래스의 경계면에 대한 함수를 학습 

k-최근접 이웃은 학습된 데이터와의 거리를 기준으로 판단

결정트리, 랜덤 포레스트, GBDT는 트리 구조로 표현된 규칙의 집합을 학습 

2.2.1 퍼셉트론 
입력 벡터와 학습한 가중치 벡터를 곱한 값을 합한 값이 0 이상일 때는 클래스 1로, 0 미만일 때는 클래스 2로 분류. 퍼셉트론을 여러 층 쌓으면 신경망이 됨

*특성
- 온라인 학습 방식을 취함(데이터를 하나하나씩 입력)
- 예측 성능은 보통이지만 학습이 빠르다
- 과접합되기 쉽다
- 선형 분리 가능한 문제만 풀 수 있다 

선형분리가능: 데이터를 직선으로 두 클래스를 분리할 수 있는 경우(직선: 초평면) 

두 클래스를 직선만으로 못 나눌때: 배타적 논리합(XOR)

*퍼셉트론의 구조
입력과 가중치를 곱한뒤 그 곱의 합을 계산
곱셈합 값이 양수인지 음수인지에 따라 클래스를 판단(이진화)

여기서 적절한 파라미터(가중치)값을 알아내려면? 
-> 실제값과 예측값이 얼마나 다른지를 나타내는 함수를 사용하면 된다. 이 함수를 손실함수(오차함수)라고 함. 이를 이용해 지금까지 학습된 예측 모델이 얼마나 좋은지를 측정할 수 있음. 
-> 힌지손실을 사용

여기서 손실함수를 어떤 모델이 데이터에 부합하는지를 나타내는 목적 함수로 만든다 
목적함수: 모든 데이터에 대한 손실 함수값의 합 
목적함수가 최소가 될 때를 최적 상태라 할 수 있다. 이러한 상태가 되는 가중치 벡터 w를 구하는 과정이 바로 '모델 학습'

*추정하는 방법?
SGD(stochastic gradient descent) 확률적 경사하강법
목적 함수의 산에서 골짜로 조금씩 내려가면서 최적의 파라미터 값을 구하는 방법
-> 파라미터를 수정해야하는데 어느정도 수정할지는 결정하는 하이퍼 파라미터를 '학습률'이라고함
파라미터 수정폭: 학습률 * 경사 
학습률이 너무 크면 수렴이 빨라지긴하지만 골짜기를 지나쳐버려 수렴이 안될 수도 있다. 반대로 너무 낮으면 수렴할때까지 너무 오래걸린다. 

활성화함수: 출력값을 비선형 변환하는 함수 
-> 계단함수를 사용

2.2.2 로지스틱 회귀
이름에 회귀가 들어가지만 사실 분류 알고리즘

*특징
- 출력과 별도로, 출력값에 해당하는 클래스에 속할 확률을 계산할 수 있음
- 온라인 학습과 배치학습이 모두 가능
- 예측 성능은 보통이며 학습 속도는 빠르다
- 과적합을 방지하는 규제항이 추가되었음 

*퍼셉트론과 로지스틱의 차이
- 로지스틱 회귀의 활성화 함수는 시그모이드함수이다
- 손실함수는 교차 엔트로피 오차 함수이다
- 규제항이 추가되어 과적합을 방지할 수 있다. 

*시그모이드 함수
하이퍼 파라미터값을 0.5로 설정함. 이것을 기준으로 원하는 성능에 따라 조절 

*규제화
학습을 수행할 때 페널티를 부여하여 결정 경계를 매끄럽게 하는 작용 
규제가 약하면 학습 데이터프를 필요 이상으로 근접
규제가 너무 강하면 하학습 데이터의 대강의 특성만 학습

규제항이 추가되면 목적함수도 달라짐
목적함수: 모든 손실함수 값+ 규제항 -> 확률적 경사하강법으로 최적화(SGD)

2.2.3 서포트 벡터 머신(SVM)
분류 문제에서 매우 많이 쓰이는 알고리즘 
선형 분리 불가능한 문제에도 사용 가능 

*특징
- 마진 최대화를 통해 매끈한 초평면 학습가능
- 커널이라는 방법을 사용하여 비선형 데이터를 분리
- 선형 커널로는 차원 수가 높은 희소 데이터도 학습
- 배치 학습와 온라인 학습에 모두 적용가능 

*결정경계
- 선형 커널은 직선
- RBF 커널은 비선형

*구조
- 손실함수: 힌지손실(퍼셉트론에서 사용한 힌지손실과는 가로축과의 교점이 다름)
결정경계에 마진이 생김

*특성
- 마진 최대화를 통해 규제화항과 비슷한 과적합 억제 효과를 얻음. 마진 최대화
기존 데이터에 대해 여유굥간을 둠. 
- 커널. 
선형 분리 불가능한 데이터에 커널 함수를 적용하여 데이터의 차원 수를 늘려, 결국 선형 분리 가능한 형태로 바꿈 
선형 커널(텍스트, 고차원 희소벡터 데이터) / 다항식 커널 / RBF 커널(이미지, 음성신호같은 조밀한 데이터)

*희소벡터: 입력 벡터의 요소 대부분이 0으로 된 벡터 

2.2.4 신경망
다층 퍼셉트론, 퍼셉트론을 하나의 노드로 하여 계층적으로 쌓아놓은 구조 

*특성
- 비선형 데이터 분리
- 학습 시간이 김
- 파라미터 수가 많으므로 과적합을 일으키기 쉬움
- 가중치의 초깃값에 민감. 지역 최적점에 빠지기 쉽다 

데이터가 많이 필요. 계산하는것이 많기 때문에 GPU를 쓰는것이 좋다

*구조
1. 3층짜리 피드포워드 신경망
입력층, 은닉층(중간층), 출력층 
입력과 가중치의 곱셈합을 구하며, 출력층에는 분류하려는 클래스의 수만큼 노드를 배치
출력층에서 계산한 값을 softmax함수로 정규화한 값을 확률로 사용하는 경우가 많음
softmax함수의 결과값이 가장 큰 클래스를 예측된 클래스로 삼음 

활성화함수: 시그모이드, ReLU 사용 

신경망 학습시 역전파 방법을 씀
-> 무작위 수로 초기화한 가중치 값을 사용하여 입력층 부터 출력층까지의 값을 계산
-> 계산한 값과 정답의 오차를 반대 방향으로 신경망을 따라 계산하며 가중치를 수정해 나감
-> 과정 끝에 가중치 변화량이 일정 수준 아래가 되거나 정해진 반복 횟수를 마치면 학습을 중단

은닉층 수를 늘리다 보면 단순히 역전파만으로 제대로 학습이 되지 않음, 이를 보안한게 딥러닝
-> 텐서플로, 체이너, 파이토치 등등 

2.2.5 k-최근접 이웃
새로운 데이터의 클래스를 다수결로 정함
이미 학습된 데이터 중 새로 입력된 데이터와 거리가 가장 가까운 k개를 선택하고 이 k개 데이터의 클래스를 확인해 가장 많은 데이터가 속한 클래스를 찾는다
+ 비슷한 항목을 탐색하는 데 사용하기도 함 

*특성
- 데이터를 하나씩 순차적으로 학습
- 기본적으로 모든 데이터와의 거리를 계산해야 하므로 예측시간이 김
- k값에 따라 편차는 있지만 예측 성능은 괜찮음
+ 정규화하여 특징들의 스케일을 맞추는 것이 필요

*결정경계
하이퍼파라미터인 k값을 어떻게 설정하느냐가 결정 경계를 좌우한다
k값이 커지면 경계가 매끈해지는 경향이 있으나 시간이 많이 걸림

*구조
k값은 교차검증을 통해 결정 -> 두 데이터 사이의 멀고 가깜움을 측정하는 '거리'를 정의해야함
가장 많이 사용되는 거리 개념: 유클리드 거리 
데이터가 분포하는 방향을 고려: 마할라노비스 거리 

*총평
거리의 척도만 정의할 수 있다면 어떤 데이터에도 적용가능 

2.2.6 결정트리, 랜덤 포레스트, GBDT

*특성
- 학습한 모델을 사람이 해석하기 쉬움
- 입력 데이터를 정규화할 필요가 없음
- 범주형 변수나 데이터의 누락값(계측 오류 등으로 값이 존재하지 않는 경우)이 있어도 용인
- 특정 조건이 맞으면 과적합을 일으키는 경향이 있음
- 비선형 문제에는 적용할 수 있지만, 선형 분리 문제는 잘 못품
- 데이터 분포가 특정 클래스에 쏠려 있으면 잘 풀지 못함
- 데이터의 작은 변화에도 결과가 크게 바뀜
- 예측 성능은 보통
- 배치 학습으로만 학습가능

트리가 깊어질수록 학습에 사용되는 적어져 과적합이 일어나기 쉬움 
-> 가지치기(pruning)으로 어느정도 방지

*구조
학습데이터로부터 조건식 -> 최상위 조건식부터 순서대로 조건 분기를 타면서 말단 조건식에 도달하면 예측결과를 냄 / 불순도(impurity)를 기준으로 가능한 한 같은 클래스끼리 모이도록 조건 분기를 학습
구체적으로 정보획득이나 지니 계수 등의 값을 불순도로 사용해 그 값이 낮아지도록 데이터 분할 

*결정트리로부터 파생된 알고리즘 

- 랜덤 포레스트
특징의 조합을 몇 가지 마련하여 성능이 좋았던 여러 개의 학습기가 내린 예측 결과를 다수결로 통합, 각각의 트리가 독립적으로 학습, 학습과정을 병렬화 가능 
가지치기를 하지 않으므로 주된 파라미터는 2개, GBDT보다는 적지만 과적합을 일으키기 쉬움. 
결정트리보다 성능이 좋고 파라미터 수가 적어 튜닝도 비교적 간단 

- GBDT 
표본추출한 데이터를 이용해 순차적으로 얕은 트리를 학습해가는 경사부스팅을 사용하는 알고리즘
예측값과 실제값의 오차를 목표변수로 삼는 방법으로 약점 보완하면서 여러개의 학습기를 학습 
오래 걸리지만 더 뛰어난 예측성능을 지님
XGBoost, LightGBM등이 속함. 특히 Kaggle에서 인기가 높다. XGBoost는 확률적인 방법으로 최적화를 수행하며 대규모 데이터도 빠르게 처리할 수 있음. LightGBM은 XGBoost보다 빠르다. 

2.3 회귀
지도 학습의 한 종류. 

*종류들
- 선형회귀: 데이터를 직선으로 근사 /  다항식 회귀: 곡선으로 근사
- 릿지 회귀: 학습한 가중치의 제곱을 규제항으로 사용   
  라쏘 회귀: 학습한 가중치의 절댓값을 규제항으로 사용 
  일래스틱넷: 선형 회귀에 두 가지 규제항을 추가 
- 회귀 트리: 결정 트리에 기초한 회귀 기법, 비선형 데이터 근사
- SVR: SVM에 기초한 회귀 기법, 비선형 데이터 근사 

2.3.1 선형 회귀의 원리
수치를 직접 출력하게 한 구조
입력 데이터를 그 제곱오차가 가장 작아지는 직선으로 근사, 이 직선의 계수를 파라미터로 구하는 것.

2.4 군집화와 차원 축소

2.4.1 군집화
비지도 학습의 한 종류. 주로 데이터의 경향성을 파악하는 데 사용되는 기법 
- 계층적 군집화
- k-mean 기법

2.4.2 차원 축소
정보를 가능한 한 온전히 보존하면서 고차원 데이터를 저차원 데이터로 변환하는 기법 

- PCA(주성분 분석)
- t-SNE 
시각화에 주로 사용, PCA보다 데이터의 관계성을 파악하기 쉬워서 캐글에서도 인기가 좋다. 

2.5 그 외

- 추천
- 이상 탐지
- 패턴 마이닝
- 강화 학습 

2.5.1 추천
사용자가 흥미 있어 할 만한 것이나, 관심을 표한 것과 비슷한 것을 미리 제시

2.5.2 이상탐지
신용카드의 부정사용이나 서비스 거부(DoS) 공격 등 이상 상태를 탐지하는 데이터 마이닝 기법
데이터 분포가 매우 극단적으로 치우쳤다는 특성 때문에 이상 탐지에는 비지도 학습 기법이 많이 사용됨. 
사이킷런에 One Class SVM

2.5.3 패턴 마이닝
데이터에서 매우 자주 발견되는 패턴을 추출 
- SPMF
시계열 분석에는 ARIMA(자기회귀 이동 평균 모델) 알고리즘이 많이 사용됨 

2.5.4 강화 학습 
많은 시행착오를 반복하며 최적의 정책을 학습하는 것이다. 
자율 주행이나 게임 인공지능 분야에서 크게 주목을 받음. 

2.6 정리
데이터의 경향을 살피며 다양한 알고리즘을 시도해볼 것. 




